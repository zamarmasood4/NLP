{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'pos_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m stop \u001b[38;5;241m=\u001b[39m StopWords_Removal(tokens_spacy)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Tag parts of speech\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mPOS_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Lemmatize tokens with POS tags\u001b[39;00m\n\u001b[0;32m     60\u001b[0m lem \u001b[38;5;241m=\u001b[39m lemmatize(pos)\n",
      "Cell \u001b[1;32mIn[48], line 40\u001b[0m, in \u001b[0;36mPOS_tag\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     38\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m---> 40\u001b[0m     pos_tags\u001b[38;5;241m.\u001b[39mappend((token, \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_\u001b[49m, token\u001b[38;5;241m.\u001b[39mtag_))\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pos_tags\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'pos_'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "from autocorrect import Speller\n",
    "spell = Speller()\n",
    "import nltk\n",
    "path='D:\\\\University Data\\\\NLP\\\\Deep Learning\\\\Text_Files' \n",
    "filepath=os.listdir(path)\n",
    "\n",
    "D_F=[]\n",
    "def filetext (filename):\n",
    "    for file in filename:\n",
    "        text=open(path+\"\\\\\"+file,\"r\").read() \n",
    "        for sentence in text:\n",
    "            D_F.append(sentence)\n",
    "text1 = filetext(filepath)\n",
    "D_S_P=[]\n",
    "for sentence in D_F:\n",
    "    D_S_P.append(sentence)\n",
    "\n",
    "def tokenize(sentences):\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "def StopWords_Removal(tokens):\n",
    "    tkns = []\n",
    "    for token in tokens:\n",
    "        if not nlp.vocab[token.text].is_stop:\n",
    "            tkns.append(token)\n",
    "    return tkns\n",
    "\n",
    "\n",
    "def POS_tag(tokens):\n",
    "    pos_tags = []\n",
    "    for token in tokens:\n",
    "        pos_tags.append((token, token.pos_, token.tag_))\n",
    "    return pos_tags\n",
    "\n",
    "\n",
    "sentences = [\"This is a sample sentence.\", \"Another sample sentence.\"]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokens = tokenize(sentences)\n",
    "\n",
    "# Convert tokens to SpaCy tokens\n",
    "tokens_spacy = [nlp(token) for token in tokens]\n",
    "\n",
    "# Remove stop words\n",
    "\n",
    "stop = StopWords_Removal(tokens_spacy)\n",
    "\n",
    "# Tag parts of speech\n",
    "pos = POS_tag(stop)\n",
    "\n",
    "# Lemmatize tokens with POS tags\n",
    "lem = lemmatize(pos)\n",
    "\n",
    "print(lem)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
